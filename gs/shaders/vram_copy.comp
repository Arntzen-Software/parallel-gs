#version 450

// SPDX-FileCopyrightText: 2024 Arntzen Software AS
// SPDX-FileContributor: Hans-Kristian Arntzen
// SPDX-FileContributor: Runar Heyer
// SPDX-License-Identifier: LGPL-3.0+

#extension GL_EXT_shader_16bit_storage : require
#extension GL_EXT_shader_8bit_storage : require
#extension GL_EXT_scalar_block_layout : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_vote : require

layout(local_size_x_id = 0) in;
layout(constant_id = 1) const uint SOURCE_FMT = 0;
layout(constant_id = 2) const uint STORAGE_FMT = 0;
layout(constant_id = 3) const uint VRAM_MASK = 4 * 1024 * 1024 - 1;
layout(constant_id = 4) const uint TRANSFER_DIR = 0;
layout(constant_id = 5) const bool INVALIDATE_SUPER_SAMPLE = false;
layout(constant_id = 6) const bool PREPARE_ONLY = false;

#include "utils.h"
#include "swizzle_utils.h"
#include "data_structures.h"

const bool is_tex_32bit = STORAGE_FMT == PSMCT32 || STORAGE_FMT == PSMZ32;
const bool is_tex_24bit = STORAGE_FMT == PSMCT24 || STORAGE_FMT == PSMZ24;
const bool is_tex_16bit = STORAGE_FMT == PSMCT16 || STORAGE_FMT == PSMCT16S ||
	STORAGE_FMT == PSMZ16 || STORAGE_FMT == PSMZ16S;
const bool is_tex_8bit = STORAGE_FMT == PSMT8 || STORAGE_FMT == PSMT8H;

layout(std430, set = 0, binding = 0) buffer VRAM32
{
	uint data[];
} vram32;

layout(std430, set = 0, binding = 0) buffer VRAM24
{
	u8vec3 data[];
} vram24;

layout(std430, set = 0, binding = 0) buffer VRAM16
{
	uint16_t data[];
} vram16;

layout(std430, set = 0, binding = 0) buffer VRAM8
{
	uint8_t data[];
} vram8;

layout(std430, set = 0, binding = 1) readonly buffer TRANSFER32
{
	uint data[];
} transfer32;

layout(scalar, set = 0, binding = 1) readonly buffer TRANSFER24
{
	u8vec3 data[];
} transfer24;

layout(std430, set = 0, binding = 1) readonly buffer TRANSFER16
{
	uint16_t data[];
} transfer16;

layout(std430, set = 0, binding = 1) readonly buffer TRANSFER8
{
	uint8_t data[];
} transfer8;

layout(std140, set = 0, binding = 2) uniform TransferDesc
{
	TransferDescriptor transfer;
};

layout(push_constant) uniform Registers
{
	uint dispatch_order;
} registers;

layout(std430, set = 0, binding = 3) buffer AtomicCounterBuffer
{
	uint link_count;
	layout(offset = PGS_LINKED_VRAM_COPY_WRITE_LIST_OFFSET) uint linked_list[];
};

layout(std430, set = 0, binding = 4) buffer LinkedPayloads
{
	LinkedVRAMCopyWrite linked_writes[];
};

// Workaround glslang issue with 8/16-bit constants and storage.
layout(constant_id = 7) const uint CONSTANT_ZERO = 0;

// Scan through the VRAM writes for a particular dword.
// If there are writes with a dispatch ID larger than our own, remove write mask.
// In pratically all cases, this should be done after one iteration to confirm that indeed,
// there are no races.
bool get_effective_write_mask(uint word_addr, uint base_mask, out uint write_mask)
{
	if (bitfieldExtract(linked_list[word_addr / 32u + (VRAM_MASK + 1) / 4], int(word_addr & 31u), 1) == 0u)
	{
		write_mask = base_mask;
		return true;
	}

	bool has_exclusive = true;
	uint link = linked_list[word_addr];
	while (link != ~0u)
	{
		uint pending_dispatch_order = bitfieldExtract(link,
			LINKED_VRAM_COPY_DISPATCH_ORDER_OFFSET,
			LINKED_VRAM_COPY_DISPATCH_ORDER_BITS);

		LinkedVRAMCopyWrite pending_write = linked_writes[bitfieldExtract(link, 0, LINKED_VRAM_COPY_DISPATCH_ORDER_OFFSET)];
		// Later writes win.
		if (pending_dispatch_order > registers.dispatch_order)
		{
			base_mask &= ~pending_write.write_mask;
			has_exclusive = false;
		}
		else if (pending_dispatch_order < registers.dispatch_order &&
			(pending_write.write_mask & ~base_mask) != 0)
		{
			// If our write doesn't invalidate all previous threads, we don't have exclusive access.
			has_exclusive = false;
		}

		link = pending_write.next;
	}

	write_mask = base_mask;
	return has_exclusive;
}

void register_vram_write(uint word_addr, uint write_mask)
{
	// Simple atomic linked list.
	// We make sure not to dispatch enough threads in one go that this can overflow buffers.
	uint link_index = atomicAdd(link_count, 1u);
	uint next_link = atomicExchange(linked_list[word_addr],
		link_index | (registers.dispatch_order << LINKED_VRAM_COPY_DISPATCH_ORDER_OFFSET));

	if (next_link != ~0u)
	{
		uint pending_dispatch_order = bitfieldExtract(next_link,
			LINKED_VRAM_COPY_DISPATCH_ORDER_OFFSET,
			LINKED_VRAM_COPY_DISPATCH_ORDER_BITS);

		if (registers.dispatch_order != pending_dispatch_order)
		{
			// Hazard within the u32. Flag this.
			atomicOr(linked_list[word_addr / 32u + (VRAM_MASK + 1) / 4], 1u << (word_addr & 31u));
		}
	}

	linked_writes[link_index] = LinkedVRAMCopyWrite(write_mask, next_link);
}

void main()
{
	ivec2 coord;
	coord.x = int(gl_WorkGroupID.x * 8 + bitfieldExtract(gl_LocalInvocationIndex, 0, 3));

	if (gl_WorkGroupSize.x == 64)
		coord.y = int(gl_WorkGroupID.y * 8 + bitfieldExtract(gl_LocalInvocationIndex, 3, 3));
	else
	{
		coord.y = int(gl_WorkGroupID.y * 4 + bitfieldExtract(gl_LocalInvocationIndex, 3, 2));
		// Skip odd-numbered addresses; both nibbles are handled in the even-numbered lookups
		coord.y = ((coord.y & ~0x1) << 1) | (coord.y & 0x1); // Bit y1 is LSB in nibble address swizzle pattern
	}

	if (coord.x >= transfer.width || coord.y >= transfer.height)
		return;

	// Deal with the case where transfer width is larger than the stride.
	// This will be a race condition with the page below, which should be written after this thread.
	// Just skip these writes unless we're writing to the last page.
	if (coord.x >= transfer.dest_stride * 64)
	{
		LocalDataStructure data_structure = get_data_structure(STORAGE_FMT);
		// This might not be technically correct if we have 2048 pixel wrapping, but ... eh.
		uint last_page = (transfer.dest_y + transfer.height - 1) >> data_structure.page_height_log2;
		uint current_page = (transfer.dest_y + coord.y) >> data_structure.page_height_log2;
		if (last_page != current_page)
			return;
	}

	// Transmission area coordinates (start + offset) wrap around at 2048
	uint source_x = (coord.x + transfer.source_x) & (2048 - 1);
	uint source_y = (coord.y + transfer.source_y) & (2048 - 1);
	uint dest_x = (coord.x + transfer.dest_x) & (2048 - 1);
	uint dest_y = (coord.y + transfer.dest_y) & (2048 - 1);

	uint source_addr;

	if (TRANSFER_DIR == HOST_TO_LOCAL)
	{
		source_addr = coord.y * transfer.width + coord.x;
	}
	else if (TRANSFER_DIR == LOCAL_TO_LOCAL)
	{
		if (!PREPARE_ONLY)
		{
			source_addr = swizzle_PS2(source_x, source_y,
				transfer.source_addr, transfer.source_stride,
				SOURCE_FMT, VRAM_MASK);
		}
		else
			source_addr = 0;
	}

	uint dest_addr = swizzle_PS2(dest_x, dest_y, transfer.dest_addr, transfer.dest_stride, STORAGE_FMT, VRAM_MASK);

	if (is_tex_24bit)
	{
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 2 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= transfer24.data.length())
			return;

		if (PREPARE_ONLY)
		{
			register_vram_write(dest_addr, 0xffffffu);
		}
		else
		{
			uint tex_data;
			if (TRANSFER_DIR == HOST_TO_LOCAL)
			{
				uvec3 bytes = uvec3(transfer24.data[source_addr]);
				tex_data = bytes.x | (bytes.y << 8) | (bytes.z << 16);
			}
			else
			{
				tex_data = uint(transfer32.data[source_addr]);
			}

			// If we have exclusive access over the dword, we don't need atomics.
			// All threads need to agree how to deal with a write to a dword.
			uint mask;
			bool has_exclusive = get_effective_write_mask(dest_addr, 0xffffffu, mask);

			if (has_exclusive)
			{
				vram24.data[dest_addr] = u8vec3(uvec3(tex_data) >> uvec3(0, 8, 16));
				if (INVALIDATE_SUPER_SAMPLE)
					vram24.data[dest_addr + (VRAM_MASK + 1) / 4] = u8vec3(uvec3(tex_data & CONSTANT_ZERO));
			}
			else if (mask != 0u)
			{
				atomicAnd(vram32.data[dest_addr], ~mask);
				atomicOr(vram32.data[dest_addr], tex_data & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[dest_addr + (VRAM_MASK + 1) / 4], 0xff000000u);
			}
		}
	}
	else if (is_tex_16bit)
	{
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 4 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= transfer16.data.length())
			return;

		uint word_addr = dest_addr >> 1u;
		uint shamt = (dest_addr & 1u) * 16u;
		uint target_write_mask = 0xffffu << shamt;

		if (PREPARE_ONLY)
		{
			register_vram_write(word_addr, target_write_mask);
		}
		else
		{
			uint write_word = uint(transfer16.data[source_addr]);
			uint mask;
			bool has_exclusive = get_effective_write_mask(word_addr, target_write_mask, mask);

			if (has_exclusive)
			{
				vram16.data[dest_addr] = uint16_t(write_word);
				if (INVALIDATE_SUPER_SAMPLE)
					vram16.data[dest_addr + (VRAM_MASK + 1) / 2] = uint16_t(write_word & CONSTANT_ZERO);
			}
			else if (mask != 0u)
			{
				write_word <<= shamt;
				atomicAnd(vram32.data[word_addr], ~mask);
				atomicOr(vram32.data[word_addr], write_word & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[word_addr + (VRAM_MASK + 1) / 4], ~target_write_mask);
			}
		}
	}
	else if (is_tex_8bit)
	{
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 8 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= transfer8.data.length())
			return;

		uint source_offset = 0;
		uint dest_offset = 0;

		if (TRANSFER_DIR == LOCAL_TO_LOCAL && SOURCE_FMT == PSMT8H)
		{
			source_addr <<= 2;
			source_offset = 3;
		}

		if (STORAGE_FMT == PSMT8H)
		{
			dest_addr <<= 2;
			dest_offset = 3;
		}

		uint byte_addr = dest_addr + dest_offset;
		uint word_addr = byte_addr >> 2u;
		uint shamt = (byte_addr & 3u) * 8u;
		uint target_write_mask = 0xffu << shamt;

		if (PREPARE_ONLY)
		{
			register_vram_write(word_addr, target_write_mask);
		}
		else
		{
			uint write_word = uint(transfer8.data[source_addr + source_offset]);
			uint mask;
			bool has_exclusive = get_effective_write_mask(word_addr, target_write_mask, mask);

			if (has_exclusive)
			{
				vram8.data[byte_addr] = uint8_t(write_word);
				if (INVALIDATE_SUPER_SAMPLE)
					vram8.data[byte_addr + (VRAM_MASK + 1)] = uint8_t(write_word & CONSTANT_ZERO);
			}
			else if (mask != 0u)
			{
				write_word <<= shamt;
				atomicAnd(vram32.data[word_addr], ~mask);
				atomicOr(vram32.data[word_addr], write_word & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[word_addr + (VRAM_MASK + 1) / 4], ~target_write_mask);
			}
		}
	}
	else if (STORAGE_FMT == PSMT4 && gl_WorkGroupSize.x == 32)
	{
		// Fused nibble approach
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 16 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= 2 * transfer8.data.length())
			return;

		uint byte_addr = dest_addr >> 1u;
		uint word_addr = byte_addr >> 2u;
		uint shamt = (byte_addr & 3u) * 8u;
		uint target_write_mask = 0xffu << shamt;

		if (PREPARE_ONLY)
		{
			register_vram_write(word_addr, target_write_mask);
		}
		else
		{
			// Find corresponding second nibble within the column so two pixels (nibbles) can be written at once
			ivec2 coord_hi = coord ^ ivec2(4, 2);
			uint write_word;

			if (TRANSFER_DIR == HOST_TO_LOCAL)
			{
				uint source_data_lo = uint(transfer8.data[source_addr >> 1]);
				uint source_addr_hi = coord_hi.y * transfer.width + coord_hi.x;
				uint source_data_hi = uint(transfer8.data[source_addr_hi >> 1]);

				if ((source_addr_hi & 1u) == 0)
					source_data_hi <<= 4;
				else
					source_data_lo >>= 4;

				write_word = bitfieldInsert(source_data_hi, source_data_lo, 0, 4);
			}
			else if (TRANSFER_DIR == LOCAL_TO_LOCAL)
			{
				if (SOURCE_FMT == PSMT4)
				{
					write_word = uint(transfer8.data[source_addr >> 1]);
				}
				else if (SOURCE_FMT == PSMT4HH || SOURCE_FMT == PSMT4HL)
				{
					int source_offset = SOURCE_FMT == PSMT4HL ? 24 : 28;
					uint source_data_lo = bitfieldExtract(transfer32.data[source_addr], source_offset, 4);
					uint pair_addr = swizzle_PS2(coord_hi.x + transfer.source_x, coord_hi.y + transfer.source_y,
					transfer.source_addr, transfer.source_stride, SOURCE_FMT, VRAM_MASK);
					uint source_data_hi = bitfieldExtract(int(transfer32.data[pair_addr]), source_offset, 4) << 4;
					write_word = bitfieldInsert(source_data_hi, source_data_lo, 0, 4);
				}
			}

			uint mask;
			bool has_exclusive = get_effective_write_mask(word_addr, target_write_mask, mask);

			if (has_exclusive)
			{
				vram8.data[byte_addr] = uint8_t(write_word);
				if (INVALIDATE_SUPER_SAMPLE)
					vram8.data[byte_addr + (VRAM_MASK + 1)] = uint8_t(write_word & CONSTANT_ZERO);
			}
			else if (mask != 0u)
			{
				write_word <<= shamt;
				atomicAnd(vram32.data[word_addr], ~mask);
				atomicOr(vram32.data[word_addr], write_word & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[word_addr + (VRAM_MASK + 1) / 4], ~target_write_mask);
			}
		}
	}
	else if (STORAGE_FMT == PSMT4)
	{
		// Fallback, sub-word atomics.
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 16 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= 2 * transfer8.data.length())
			return;

		uint word_addr = dest_addr >> 3u;
		uint shamt = 4u * (dest_addr & 7u);
		uint target_write_mask = 0xfu << shamt;

		if (PREPARE_ONLY)
		{
			register_vram_write(word_addr, target_write_mask);
		}
		else
		{
			uint mask;
			get_effective_write_mask(word_addr, target_write_mask, mask);

			if (mask != 0u)
			{
				uint source_data;
				if (SOURCE_FMT == PSMT4 || TRANSFER_DIR == HOST_TO_LOCAL)
				{
					source_data = uint(transfer8.data[source_addr >> 1]);
					source_data = bitfieldExtract(source_data, 4 * int(source_addr & 1u), 4);
				}
				else
				{
					int source_offset = SOURCE_FMT == PSMT4HL ? 24 : 28;
					source_data = bitfieldExtract(transfer32.data[source_addr], source_offset, 4);
				}

				source_data <<= shamt;
				atomicAnd(vram32.data[word_addr], ~mask);
				atomicOr(vram32.data[word_addr], source_data & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[word_addr + (VRAM_MASK + 1) / 4], ~mask);
			}
		}
	}
	else if (STORAGE_FMT == PSMT4HL || STORAGE_FMT == PSMT4HH)
	{
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 16 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= transfer8.data.length() * 2)
			return;

		int dest_offset = STORAGE_FMT == PSMT4HL ? 24 : 28;
		uint target_write_mask = 0xfu << dest_offset;

		if (PREPARE_ONLY)
		{
			register_vram_write(dest_addr, target_write_mask);
		}
		else
		{
			uint mask;
			get_effective_write_mask(dest_addr, target_write_mask, mask);

			if (mask != 0u)
			{
				uint source_data;
				if (TRANSFER_DIR == HOST_TO_LOCAL)
				{
					source_data = bitfieldExtract(uint(transfer8.data[source_addr >> 1u]), int(source_addr & 1u) * 4, 4);
				}
				else if (TRANSFER_DIR == LOCAL_TO_LOCAL)
				{
					if (SOURCE_FMT == PSMT4)
					{
						int source_offset = int(source_addr & 1u) << 2;
						source_data = bitfieldExtract(uint(transfer8.data[source_addr >> 1]), source_offset, 4);
					}
					else
					{
						int source_offset = SOURCE_FMT == PSMT4HL ? 24 : 28;
						source_data = bitfieldExtract(transfer32.data[source_addr], source_offset, 4);
					}
				}

				source_data <<= dest_offset;
				atomicAnd(vram32.data[dest_addr], ~mask);
				atomicOr(vram32.data[dest_addr], source_data & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[dest_addr + (VRAM_MASK + 1) / 4], ~mask);
			}
		}
	}
	else
	{
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr * 2 < transfer.host_offset_qwords)
			return;
		if (TRANSFER_DIR == HOST_TO_LOCAL && source_addr >= transfer32.data.length())
			return;

		if (PREPARE_ONLY)
		{
			register_vram_write(dest_addr, ~0u);
		}
		else
		{
			uint mask;
			bool exclusive_write = get_effective_write_mask(dest_addr, ~0u, mask);
			uint tex_word = transfer32.data[source_addr];

			if (exclusive_write)
			{
				vram32.data[dest_addr] = tex_word;
				if (INVALIDATE_SUPER_SAMPLE)
					vram32.data[dest_addr + (VRAM_MASK + 1) / 4] = 0;
			}
			else if (mask != 0u)
			{
				atomicAnd(vram32.data[dest_addr], ~mask);
				atomicOr(vram32.data[dest_addr], tex_word & mask);
				if (INVALIDATE_SUPER_SAMPLE)
					atomicAnd(vram32.data[dest_addr + (VRAM_MASK + 1) / 4], 0u);
			}
		}
	}
}