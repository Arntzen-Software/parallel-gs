#version 450

// SPDX-FileCopyrightText: 2024 Arntzen Software AS
// SPDX-FileContributor: Hans-Kristian Arntzen
// SPDX-FileContributor: Runar Heyer
// SPDX-License-Identifier: LGPL-3.0+

#extension GL_EXT_shader_explicit_arithmetic_types_int16 : require
#extension GL_EXT_scalar_block_layout : require

//#extension GL_EXT_spirv_intrinsics : require
//spirv_execution_mode(extensions = ["SPV_KHR_float_controls"], capabilities = [4467], 4462, 32);

#define USE_RCP_FIXED
#define USE_RCP_FLOAT
#define PRIMITIVE_LIST_QUALIFIER readonly
#define PRIMITIVE_SETUP_QUALIFIER writeonly

#define NEED_VERTEX_POSITION
#define NEED_VERTEX_ATTRIBUTE
#define NEED_PRIMITIVE_SETUP
#define NEED_PRIMITIVE_ATTRIBUTE
#define NEED_TRANSFORMED_ATTRIBUTE
#define NEED_STATE_VECTORS

#include "data_buffers.h"
#include "data_structures.h"
#include "math_utils.h"
#include "intersect.h"
#include "utils.h"

layout(local_size_x = 64) in;

layout(constant_id = 0) const int SAMPLING_RATE_X_LOG2 = 0;
layout(constant_id = 1) const int SAMPLING_RATE_Y_LOG2 = 0;
layout(constant_id = 2) const bool SUPER_SAMPLED_TEXTURES = false;
layout(constant_id = 3) const bool FORCE_LINEAR_SUPER_SAMPLE = false;

layout(push_constant) uniform Registers
{
    uint num_primitives;
    uint packed_z_shift; // Shifts depth into 32-bit buckets.
    uint rp_is_blur_mask;
} registers;

layout(set = 0, binding = BINDING_SINGLE_SAMPLE_HEURISTIC, std430) buffer SingleSampleHeuristicSSBO
{
    SingleSampleHeuristic data;
} heuristic;

layout(set = 0, binding = BINDING_OPAQUE_FBMASKS, std430) uniform FBMasks
{
    uint opaque_fbmasks[8];
};

bool tie_break_rule(ivec2 edge)
{
    return (edge.y > 0) || (edge.y == 0 && edge.x < 0);
}

int shift_int64_subpixels(ivec2 c)
{
    int msb = c.y << (32 - PGS_SUBPIXEL_RASTER_BITS);
    int lsb = int(uint(c.x) >> PGS_SUBPIXEL_RASTER_BITS);
    return msb | lsb;
}

ivec3 quantize_step(int a, int b, ivec2 c)
{
    return ivec3(a, b, shift_int64_subpixels(c));
}

bool order_less_than(ivec2 a, ivec2 b, uint az, uint bz)
{
    if (bz != az)
        return az < bz;
    else if (b.y != a.y)
        return a.y < b.y;
    else
        return a.x < b.x;
}

void swap(inout ivec2 x, inout ivec2 y)
{
    ivec2 tmp = x;
    x = y;
    y = tmp;
}

void swap(inout uint x, inout uint y)
{
    uint tmp = x;
    x = y;
    y = tmp;
}

ivec2 smul32x32(int a, int b)
{
    ivec2 result;
    imulExtended(a, b, result.y, result.x);
    return result;
}

ivec2 sub64(ivec2 a, ivec2 b)
{
    uint borrow;
    uint lsb = usubBorrow(a.x, b.x, borrow);
    uint msb = a.y - b.y - borrow;
    return ivec2(lsb, msb);
}

vec2 unpack_uv(u16vec2 uv)
{
    return vec2(uv) / float(PGS_SUBPIXELS);
}

bvec2 logicalOr(bvec2 a, bvec2 b)
{
    return bvec2(a.x || b.x, a.y || b.y);
}

bvec2 logicalAnd(bvec2 a, bvec2 b)
{
    return bvec2(a.x && b.x, a.y && b.y);
}

void main_inner(uint index)
{
    ivec3 order = ivec3(0, 1, 2);

    uint state_word = primitive_attr.data[index].state;
    bool parallelogram = bitfieldExtract(state_word, STATE_BIT_PARALLELOGRAM, 1) != 0;
    bool perspective = bitfieldExtract(state_word, STATE_BIT_PERSPECTIVE, 1) != 0;
    bool iip = bitfieldExtract(state_word, STATE_BIT_IIP, 1) != 0;
    bool fix = bitfieldExtract(state_word, STATE_BIT_FIX, 1) != 0;
    bool sprite = bitfieldExtract(state_word, STATE_BIT_SPRITE, 1) != 0;
    bool line = bitfieldExtract(state_word, STATE_BIT_LINE, 1) != 0;
    bool multisample = state_is_multisample(state_word);
    uint provoking = bitfieldExtract(state_word, STATE_PARALLELOGRAM_PROVOKING_OFFSET, STATE_PARALLELOGRAM_PROVOKING_COUNT);

    uint instance = bitfieldExtract(state_word, STATE_VERTEX_RENDER_PASS_INSTANCE_OFFSET, STATE_VERTEX_RENDER_PASS_INSTANCE_COUNT);
    uint opaque_fbmask = opaque_fbmasks[instance];

    VertexPosition v0 = vertex_position.data[3u * index + 0u];
    VertexPosition v1 = vertex_position.data[3u * index + 1u];
    VertexPosition v2 = vertex_position.data[3u * index + 2u];
    ivec2 a = v0.pos;
    ivec2 b = v1.pos;
    ivec2 c = v2.pos;
    uint za = v0.z;
    uint zb = v1.z;
    uint zc = v2.z;
    ivec4 primitive_bb = ivec4(primitive_attr.data[index].bb);

    // Rough approximation of bresenham. We need to handle the case where primitives expect exit rule.
    // The simplest workaround is just to offset by one sub-pixel if we observe edge case.
    // Need hardware tests to determine exactly how the rules go.
    if (line)
    {
        if (b.x > a.x)
        {
            a.x += 1;
            b.x += 1;
        }

        if (b.y > a.y)
        {
            a.y += 1;
            b.y += 1;
        }
    }

    if ((!parallelogram || line) && !fix)
    {
        // Unsure if PS2 primitives are rotationally invariant.
        // They probably are as DDA tends to sort vertices.
        // Parallelograms depend on their provoking vertex, so don't do anything funny here.
        // Select provoking vertex as top, then left.
        // We want lowest Z to be provoking to increase Z interpolation accuracy, especially in 32-bit Z.
        if (!line && !order_less_than(b, c, zb, zc))
        {
            order = order.xzy;
            swap(b, c);
            swap(zb, zc);
        }

        if (!order_less_than(a, b, za, zb))
        {
            order = order.yxz;
            swap(a, b);
            swap(za, zb);
        }
    }
    else if (provoking == 1)
    {
        order = order.yxz;
        swap(a, b);
        swap(za, zb);
    }
    else if (provoking == 2)
    {
        order = order.zyx;
        swap(a, c);
        swap(za, zc);
    }

    if (sprite)
    {
        c = ivec2(a.x, b.y);
        b = ivec2(b.x, a.y);
    }
    else if (line)
    {
        // For now, construct a simple parallelogram line.
        ivec2 ab = b - a;
        ivec2 abs_d = abs(ab);

        if (abs_d.x > abs_d.y)
        {
            // Horiz major. Ensure area is positive so we don't have to deal with reordering.
            // Make sure the width is exactly 1 pixel, and tie-break rules ensure we only get one pixel
            // hit. This emulates Bresenham rather well.
            int y_delta = -sign(ab.x) << (PGS_SUBPIXEL_BITS - 1);
            c.x = a.x;
            c.y = a.y + y_delta;
            a.y -= y_delta;
            b.y -= y_delta;
        }
        else
        {
            // Vert major. Ensure area is positive so we don't have to deal with reordering.
            int x_delta = sign(ab.y) << (PGS_SUBPIXEL_BITS - 1);
            c.x = a.x + x_delta;
            c.y = a.y;
            a.x -= x_delta;
            b.x -= x_delta;
        }
    }

    // Y or Z is selected as i/j target based on winding later.
    ivec2 ab = b - a;
    ivec2 ac = c - a;

    ivec2 signed_area = sub64(smul32x32(ab.y, ac.x), smul32x32(ab.x, ac.y));

    if (all(equal(signed_area, ivec2(0))))
    {
        // Degenerate, no pixels are rasterized.
        primitive_setup.data[index].bb = ivec4(0, 0, -1, -1);

        // Make sure that degenerate primitives don't contribute to breaking heuristic.
        if (SAMPLING_RATE_Y_LOG2 != 0)
            primitive_attr.data[index].state |= 1u << STATE_BIT_SNAP_ATTRIBUTE;

        return;
    }

    // Signed area has a range of +/- 0xffff * 0xffff.

    uint area;

    if (signed_area.y < 0)
    {
        area = uint(-signed_area.x);
        swap(ab, ac);
        swap(b, c);
        swap(zb, zc);
        order = order.xzy;
    }
    else
    {
        area = uint(signed_area.x);
    }

    ivec2 bc = c - b;
    ivec2 ca = -ac;

    int dcdx = +ab.y;
    int dcdy = -ab.x;
    int dadx = +bc.y;
    int dady = -bc.x;
    int dbdx = +ca.y;
    int dbdy = -ca.x;

    ivec2 constant_c = sub64(smul32x32(-a.x, dcdx), smul32x32(a.y, dcdy));
    ivec2 constant_a = sub64(smul32x32(-b.x, dadx), smul32x32(b.y, dady));
    ivec2 constant_b = sub64(smul32x32(-c.x, dbdx), smul32x32(c.y, dbdy));

    bool tie_break_a = tie_break_rule(bc);
    bool tie_break_b = tie_break_rule(ca);
    bool tie_break_c = tie_break_rule(ab);
    int error_a = int(!tie_break_a);
    int error_b = int(!tie_break_b);
    int error_c = int(!tie_break_c);

    constant_a = sub64(constant_a, ivec2(error_a, 0));
    constant_b = sub64(constant_b, ivec2(error_b, 0));
    constant_c = sub64(constant_c, ivec2(error_c, 0));

    error_a += constant_a.x & (PGS_SUBPIXELS_RASTER - 1);
    error_b += constant_b.x & (PGS_SUBPIXELS_RASTER - 1);
    error_c += constant_c.x & (PGS_SUBPIXELS_RASTER - 1);

    int error_flip_b = 0;
    int error_flip_c = 0;
    if (parallelogram)
    {
        int tie_break_offset = 1 - int(tie_break_b) - int(tie_break_c);
        error_flip_b = tie_break_offset + error_b + error_a;
        error_flip_c = tie_break_offset + error_c + error_a;
        error_flip_b += PGS_SUBPIXELS_RASTER; // Ensure positive result. Compensate in raster.
        error_flip_c += PGS_SUBPIXELS_RASTER; // Ensure positive result. Compensate in raster.
        error_flip_b >>= PGS_SUBPIXEL_RASTER_BITS;
        error_flip_c >>= PGS_SUBPIXEL_RASTER_BITS;
    }

    float inv_area = rcp_fixed(area);
    precise float inv_area_scaled = inv_area * float(PGS_SUBPIXELS_RASTER);
    precise float error_i = float(error_b) * inv_area;
    precise float error_j = float(error_c) * inv_area;

    if (sprite)
    {
        zb = za;
        zc = za;
    }
    else if (line)
    {
        zc = za;
    }

    if (fix || sprite)
        order = ivec3(0, 1, 2);

    VertexAttribute attr0 = vertex_attr.data[3u * index + order.x];
    VertexAttribute attr1 = vertex_attr.data[3u * index + order.y];
    VertexAttribute attr2;
    if (!sprite && !line)
        attr2 = vertex_attr.data[3u * index + order.z];

    vec4 stqf0, stqf1, stqf2;
    if (perspective)
    {
        stqf0 = vec4(attr0.st, attr0.q, attr0.fog);
        stqf1 = vec4(attr1.st, attr1.q, attr1.fog);
        if (!sprite && !line)
            stqf2 = vec4(attr2.st, attr2.q, attr2.fog);
    }
    else
    {
        stqf0 = vec4(unpack_uv(attr0.uv), attr0.q, attr0.fog);
        stqf1 = vec4(unpack_uv(attr1.uv), attr1.q, attr1.fog);
        if (!sprite && !line)
            stqf2 = vec4(unpack_uv(attr2.uv), attr2.q, attr2.fog);
    }

    // +U follows X, +V follows Y it seems ...
    if (sprite)
    {
        // Need to apply the YZ flip ourselves since we're synthesizing the attributes.
        if (signed_area.y < 0)
        {
            stqf2 = vec4(stqf1.x, stqf0.yzw);
            stqf1 = vec4(stqf0.x, stqf1.y, stqf0.zw);
        }
        else
        {
            stqf2 = vec4(stqf0.x, stqf1.y, stqf0.zw);
            stqf1 = vec4(stqf1.x, stqf0.yzw);
        }
    }
    else if (line)
    {
        stqf2 = stqf0;
    }

    if (perspective)
    {
        // Negative Q is allowed, but they all have to match sign.
        // This should be fine.
        if (stqf0.z < 0.0)
        {
            stqf0.xy = -stqf0.xy;
            stqf1.xy = -stqf1.xy;
            stqf2.xy = -stqf2.xy;
        }

        stqf0.z = abs(stqf0.z);
        stqf1.z = abs(stqf1.z);
        stqf2.z = abs(stqf2.z);

        uint qbits = floatBitsToUint(max(max(stqf0.z, stqf1.z), stqf2.z));
        uint qexp = bitfieldExtract(qbits, 23, 8);

        // 8-bit mantissa for Q is rounded down.
        stqf0.z = uintBitsToFloat(floatBitsToUint(stqf0.z) & ~0xffu);
        stqf1.z = uintBitsToFloat(floatBitsToUint(stqf1.z) & ~0xffu);
        stqf2.z = uintBitsToFloat(floatBitsToUint(stqf2.z) & ~0xffu);

        // Apparently ST is rounded depending on how large Q is.
        // This suggests a re-scaling operation into fixed point in hardware.
        // This would make a lot of sense, since interpolating in FP on a DDA archicture would be exceptionally strange.
        // This exact formulation is very unlikely to be HW accurate, but it feels like a reasonable approximation
        // if I had to write a fixed point interpolator myself.
        // With rescaling and flipping signs (Q has to have uniform sign),
        // Q could be represented in 16-bit uint with 0x8000 representing 2^n, with 15 mantissa bits left from the quantize.
        uint sexp = bitfieldExtract(floatBitsToUint(max(max(abs(stqf0.x), abs(stqf1.x)), abs(stqf2.x))), 23, 8);
        uint texp = bitfieldExtract(floatBitsToUint(max(max(abs(stqf0.y), abs(stqf1.y)), abs(stqf2.y))), 23, 8);

        // 9 bits is apparently suggested by HW tests. I cannot confirm or verify this.
        // It does make sense if we consider that S/T might be quantized to 16-bit sint.
        // If we take into account that UV should be clamped to +/- 2047.0, these results start making more sense.
        // Consider 1 bit for sign, 0x4000 marking 1.0, and 14 mantissa bits left.
        // S = 1.0 + 2^-14, Q = 1.0 could be represented as 0x4001.
        // For large S, we could store an extra shift factor. We already shift up the result by TW/TH,
        // so folding that exponential shift in for normalized textures make a lot of sense.
        // For small S, S could be scaled in fixed point so the shifter can be a pure left shift.
        // Seems reasonable from a hardware design PoV.
        uint schop = min(9 + max(sexp, qexp) - sexp, 23);
        uint tchop = min(9 + max(texp, qexp) - texp, 23);
        uint smask = (1u << schop) - 1u;
        uint tmask = (1u << tchop) - 1u;
        uvec2 stmask = ~uvec2(smask, tmask);

        stqf0.xy = uintBitsToFloat(floatBitsToUint(stqf0.xy) & stmask);
        stqf1.xy = uintBitsToFloat(floatBitsToUint(stqf1.xy) & stmask);
        stqf2.xy = uintBitsToFloat(floatBitsToUint(stqf2.xy) & stmask);

        // For trivial perspective, we should strive for maximum accuracy, since some games rely on that.
        // Resolve the perspective in highest possible accuracy.
        // With fixed RCP, we can get essentially 24.0 bits, but without we have to rely on ~22.5 bit RCP
        // with different behavior per vendor.
        //
        // Technically, we should do perspective divider after interpolation,
        // but this is at least mathematically equivalent.
        // This level of accuracy only seems to matter in extreme single STQ value scenarios anyway.
        uint tex2_state = primitive_attr.data[index].tex2;
        bool needs_lod_computation = bitfieldExtract(tex2_state, TEX2_FIXED_LOD_OFFSET, TEX2_FIXED_LOD_BITS) == 0;
        if (!needs_lod_computation && stqf0.z == stqf1.z && stqf0.z == stqf2.z)
        {
            float inv_q = rcp_float(stqf0.z);
            stqf0.xy *= inv_q;
            stqf1.xy *= inv_q;
            stqf2.xy *= inv_q;
            stqf0.z = 1.0;
            stqf1.z = 1.0;
            stqf2.z = 1.0;
        }
    }

    precise vec4 distqf = fix ? vec4(0.0) : (stqf1 - stqf0);
    precise vec4 djstqf = fix ? vec4(0.0) : (stqf2 - stqf0);

    transformed_attr.data[index].stqf0 = stqf0;
    transformed_attr.data[index].stqf1 = distqf;
    transformed_attr.data[index].stqf2 = djstqf;

    if (iip && !sprite)
    {
        if (fix)
            attr1.rgba = attr0.rgba;
        if (fix || line)
            attr2.rgba = attr0.rgba;
        transformed_attr.data[index].rgba0 = attr0.rgba;
        transformed_attr.data[index].rgba1 = attr1.rgba;
        transformed_attr.data[index].rgba2 = attr2.rgba;
    }
    else
    {
        uint flat_rgba = vertex_attr.data[3u * index].rgba;
        transformed_attr.data[index].rgba0 = flat_rgba;
        transformed_attr.data[index].rgba1 = flat_rgba;
        transformed_attr.data[index].rgba2 = flat_rgba;
    }

    transformed_attr.data[index].padding = 0; // Avoid striped cache-line writeback.

    ivec3 plane_a = quantize_step(dadx, dady, constant_a);
    ivec3 plane_b = quantize_step(dbdx, dbdy, constant_b);
    ivec3 plane_c = quantize_step(dcdx, dcdy, constant_c);

    plane_a.x = bitfieldInsert(plane_a.x << 6, int(parallelogram), 0, 1);
    plane_a.x = bitfieldInsert(plane_a.x, error_flip_b, 2, 2);
    plane_a.x = bitfieldInsert(plane_a.x, error_flip_c, 4, 2);

    ivec2 d = parallelogram ? (b + c - a) : c;

    // Highly speculative, but appears to be needed.
    // When DDA attempts to reconstruct the top-left sprite corner, it seems like we may need to consider rounding
    // errors. If we suspect rounding errors are at play, bias the interpolation at epsilon offset to
    // fake it towards the real input vertex.
    if (sprite)
    {
        // D is the first vertex in the sprite. B and C are "fake" parallelogram vertices.
        int x_dist = d.x - a.x;
        int y_dist = d.y - a.y;

        // Only adjust if NPOT, since POT scaling we expect lossless varying interpolation.
        bool right_major_adjust = x_dist > 0 && (x_dist & (x_dist - 1)) != 0;
        bool bottom_major_adjust = y_dist > 0 && (y_dist & (y_dist - 1)) != 0;

        int di_rounding = 0;
        int dj_rounding = 0;

        if (right_major_adjust)
        {
            di_rounding += plane_b.x;
            dj_rounding += plane_c.x;
        }

        if (bottom_major_adjust)
        {
            di_rounding += plane_b.y;
            dj_rounding += plane_c.y;
        }

        // 1/64th of a pixel offset in interpolation. Arbitrary choice that is enough to fix issues
        // since the ubershader applies a tiny epsilon on UVs.
        // We don't risk sampling out of bounds on the bottom-right part of the sprite
        // since sampling at the bottom-right edge does not have coverage. Convenient.
        error_i += float(di_rounding) * (0.125 * inv_area_scaled);
        error_j += float(dj_rounding) * (0.125 * inv_area_scaled);
    }

    primitive_setup.data[index].a = plane_a;
    primitive_setup.data[index].inv_area = inv_area_scaled;
    primitive_setup.data[index].b = plane_b;
    primitive_setup.data[index].error_i = error_i;
    primitive_setup.data[index].c = plane_c;
    primitive_setup.data[index].error_j = error_j;
    // This will be fine as long as we assume that the primitive doesn't have an insane range in 32-bit Z.
    // We get ideal accuracy on the lowest Z, i.e. closer to far plane, which should be ideal, since that's where
    // precision is needed.
    primitive_setup.data[index].z = uvec4(za, floatBitsToUint(float(zb - za)), floatBitsToUint(float(zc - za)), 0);

    ivec4 packed_bb = pack_bounding_box(a, b, c, d, multisample, SAMPLING_RATE_Y_LOG2);

    packed_bb = clip_bounding_box(packed_bb, ivec4(primitive_attr.data[index].bb));
    primitive_setup.data[index].bb = packed_bb;

    if (sprite && !perspective)
    {
        // In some odd cases, a game really should have used NEAREST filter.
        // Detecting this is important since at least one game in the wild seems to barely touch the neighbor sub-texel
        // wreacking havok in channel shuffle effects.
        // Resolving this with pure interpolation requires near-infinite precision, which we don't have.
        ivec2 uv_delta0 = ivec2(attr0.uv) - a;
        ivec2 uv_delta1 = ivec2(attr1.uv) - d;
        ivec2 min_delta = min(uv_delta0, uv_delta1);
        ivec2 max_delta = max(uv_delta0, uv_delta1);

        // A single ULP of FP error rounding down should cause the maximum delta to snap to texel center.
        bvec2 center_snapped = equal(min_delta & (PGS_SUBPIXELS - 1), ivec2(PGS_SUBPIXELS >> 1));

        // Only consider this case if the scaling is NPOT. Otherwise, we can expect perfect scaling.
        bvec2 next_snapped = equal(max_delta & (PGS_SUBPIXELS - 1), ivec2(PGS_SUBPIXELS >> 1) + 1);
        ivec2 dist = abs(d - a);
        bvec2 scaling_is_npot = notEqual(dist & (dist - 1), ivec2(0));
        next_snapped = logicalAnd(next_snapped, scaling_is_npot);

        // If this is true, we assume that the value is always between min and max.
        // Since it seems like the DDA rounds down, we always get the minimum value.
        bool no_effective_interpolation = all(lessThanEqual(max_delta - min_delta, ivec2(1)));

        // A dimension has to be center snapped or next snapped. Some constellation that should disable LINEAR:
        // - Lerp center to center.
        //   This is a perfect noop. Assume that game is not relying on rounding behavior, since that would be very weird.
        //   For POT scaling factors, this should hold nicely.
        // - Lerp center to center + 1/16.
        //   Since the result cannot be expected to ever reach >= center + 1/16, round down to center always.
        // - Lerp center + 1/16 to center + 1/16.
        //   This one is more esoteric, but if the scaling factor is non-POT, the DDA might step a bit too short
        //   every time, leading to the effective UV rounding down to center anyway.
        //   Interpolating at exactly 1/16 offset like this is esoteric at best, so assume the game did not intend that.
        bool half_texel_offset = all(logicalOr(next_snapped, center_snapped));

        // Alternative check. If a sprite has a 1/16th offset in both corners, it seems like
        // the DDA interpolation tends round down, leading to a snapped interpolation anyway.
        // Workarounds real-world game issues that seem to run fine on real hardware,
        // so this seems like a plausible root cause.

        // If we only deviate by one sub-texel, interpolation will very likely never hit that upper value.
        // Since we snap down to the minimum value either way.
        if (half_texel_offset && no_effective_interpolation)
            primitive_attr.data[index].tex &= ~TEX_SAMPLER_MAG_LINEAR_BIT;
    }

    // We don't overwrite all active bits in the render pass. This is not an opaque primitive after all.
    uint state = primitive_attr.data[index].state;
    if (primitive_attr.data[index].fbmsk != opaque_fbmask && (state & (1u << STATE_BIT_OPAQUE)) != 0)
        primitive_attr.data[index].state = state & ~(1u << STATE_BIT_OPAQUE);

    // Only allow per-sample texturing for cases where filtering will not cause issues.
    if (SAMPLING_RATE_Y_LOG2 != 0 && SUPER_SAMPLED_TEXTURES)
    {
        uint tex = primitive_attr.data[index].tex;

        // If we have already decided on sample mapping, just do that.
        if ((tex & (TEX_PER_SAMPLE_BIT | TEX_SAMPLE_MAPPING_BIT)) == TEX_PER_SAMPLE_BIT)
        {
            ivec2 uv_delta = abs(ivec2(attr1.uv) - ivec2(attr0.uv));
            ivec2 dist = abs(d - a);
            bool is_perspective = (state & (1u << STATE_BIT_PERSPECTIVE)) != 0;

            if ((tex & TEX_SAMPLER_MAG_LINEAR_BIT) == 0)
            {
                if (is_perspective)
                {
                    // If we promoted a super-sampled perspective quad, always assume sample mapping,
                    // since we won't do any analysis on the UVs.
                    tex |= TEX_SAMPLE_MAPPING_BIT;
                }
                else if (any(greaterThan(uv_delta, dist + 1)))
                {
                    // For nearest scaling, avoid broken filter kernels on downsampling.
                    // For direct blits, force per-sample mapping, irregardless of the sampling phase.
                    // Just use the normal super-sampled texture. It's already anti-aliased.
                    tex &= ~TEX_PER_SAMPLE_BIT;
                }
                else if (any(greaterThanEqual(uv_delta, dist - PGS_SUBPIXELS)))
                {
                    // Be quite conservative with the allowed range since some games require this.
                    uint state_index = state_get_index(state);
                    uint blend = state_vectors.data[state_index].blend_mode;

                    uint rp_instance = bitfieldExtract(state,
                        STATE_VERTEX_RENDER_PASS_INSTANCE_OFFSET,
                        STATE_VERTEX_RENDER_PASS_INSTANCE_COUNT);
                    bool rp_is_blur = bitfieldExtract(
                        registers.rp_is_blur_mask, int(rp_instance), 1) != 0;

                    // Heuristic if we're doing additive blending of some kind.
                    // A more clever heuristic would be to detect if there are
                    // multiple blends from the same texture at different phases.
                    // We want to avoid the heuristic if we're only blending 1:1 on top.
                    bool blur_kernel =
                        rp_is_blur &&
                        (blend & BLEND_MODE_ABE_BIT) != 0 && (state & (1 << STATE_BIT_Z_TEST)) == 0 &&
                        bitfieldExtract(blend, BLEND_MODE_A_MODE_OFFSET, BLEND_MODE_A_MODE_BITS) == BLEND_RGB_SOURCE &&
                        bitfieldExtract(blend, BLEND_MODE_B_MODE_OFFSET, BLEND_MODE_B_MODE_BITS) == BLEND_RGB_ZERO &&
                        bitfieldExtract(blend, BLEND_MODE_D_MODE_OFFSET, BLEND_MODE_D_MODE_BITS) == BLEND_RGB_DEST;

                    if (blur_kernel)
                    {
                        if (FORCE_LINEAR_SUPER_SAMPLE)
                        {
                            // Deduce the game is trying to do a blur kernel. Promote this to super-sampled against LOD 0.
                            // Instead of all samples sampling at a specific coordinate, all samples will now sample different
                            // coordinates. We want the average coordinate for those samples to be equal to the original,
                            // to avoid unexpected image "shifts".
                            tex |= TEX_SAMPLE_RESOLVED_BIT | TEX_SAMPLER_MAG_LINEAR_BIT;

                            // Adjust the phase so that the average sampling coordinate maps to the center of the texel.
                            // With nearest sampling, a game can use any phase within the texels and still expect
                            // the same result. Since we're promoting back to LINEAR we need to make
                            // sure we sample at texel centers on average to avoid phase shifts.
                            ivec2 average_uv = (ivec2(attr0.uv) + ivec2(attr1.uv)) >> 1;
                            ivec2 average_pos = (d + a) >> 1;

                            ivec2 phase = (average_uv - average_pos) & (PGS_SUBPIXELS - 1);
                            stqf0.xy += 0.5 - vec2(phase) / PGS_SUBPIXELS;
                            transformed_attr.data[index].stqf0.xy = stqf0.xy;
                        }
                        else
                        {
                            // Demote to single-sampled.
                            // Blur kernels are extremely tricky to get right.
                            tex &= ~TEX_PER_SAMPLE_BIT;
                        }
                    }
                    else
                    {
                        // Most likely it's a feedback that requires 1:1 sample mapping to work.
                        tex |= TEX_SAMPLE_MAPPING_BIT;
                    }
                }
            }
            else if (!is_perspective)
            {
                // Detect common blur stages.
                // By default, any filtering in the low-res domain should stay there.
                // There are too many glitches involved.
                bool is_downsample = all(greaterThan(uv_delta, dist + 1));
                bool is_upsample = all(greaterThanEqual(dist, 2 * uv_delta));
                if (is_downsample)
                {
                    if (!FORCE_LINEAR_SUPER_SAMPLE)
                    {
                        // Demote LINEAR filtering to base layer.
                        tex &= ~TEX_PER_SAMPLE_BIT;
                    }
                }
                else if (is_upsample)
                {
                    // When upscaling, we should sample the resolved layer,
                    // since we expect there has been a previous downsampling,
                    // and downsampling forces single-sampled render.
                    // We can use super-sampled UV here since upscaling tends to
                    // sample at 0.25 and 0.75 phase. This blends against the next pixel
                    // and adding super sampling will not cause additional bleed,
                    // so this should be safe in any reasonable situation.
                    if (!FORCE_LINEAR_SUPER_SAMPLE)
                        tex |= TEX_SAMPLE_RESOLVED_BIT;
                }

                // In the in-between zone that isn't obviously downsample or upsample,
                // we deduce that the game is trying to either do "blit-to-FB" passes,
                // or non-blur related filtering.
                // This is most likely going to be safe it seems.
            }

            primitive_attr.data[index].tex = tex;

            if ((tex & (TEX_PER_SAMPLE_BIT | TEX_SAMPLE_MAPPING_BIT)) == TEX_PER_SAMPLE_BIT)
            {
                // We've opted for a straight forward super-sampled texture. Force per-sample interpolation.
                const uint SNAP_ATTR_BIT = (1u << STATE_BIT_SNAP_ATTRIBUTE);
                primitive_attr.data[index].state &= ~SNAP_ATTR_BIT;

                // Adjust i/j interpolation such that we get an average sampling position
                // equal to single sampled result. This combats any unexpected shifts.
                vec2 bottom_right_sample =
                    float(1 << PGS_RASTER_SUBSAMPLE_BITS) *
                    get_average_sampling_offset(SAMPLING_RATE_X_LOG2, SAMPLING_RATE_Y_LOG2);

                ivec2 b = plane_b.xy;
                ivec2 c = plane_c.xy;
                float di_dcorner = float(b.x + b.y) * bottom_right_sample.x;
                float dj_dcorner = float(c.x + c.y) * bottom_right_sample.y;
                primitive_setup.data[index].error_i = error_i - di_dcorner * inv_area_scaled;
                primitive_setup.data[index].error_j = error_j - dj_dcorner * inv_area_scaled;
            }
        }
    }

    if (SAMPLING_RATE_Y_LOG2 != 0 && !sprite)
    {
        // Try to detect common patterns of flat UI primitives.
        // In this case, we should aim to snap attribute interpolation to avoid common glitches.
        // There is no point in upsampling UI elements anyway.
        // Filtering it further will only look worse and less sharp than the native alternative.
        bool should_interpolate_per_sample = multisample;

        // If there is any proof of 3D-ness, like perspective, or different Z values, we're most likely rendering 3D.
        if (!should_interpolate_per_sample && perspective)
            should_interpolate_per_sample = distqf.z != 0.0 || djstqf.z != 0.0;
        if (!should_interpolate_per_sample)
            should_interpolate_per_sample = za != zb || zb != zc;

        if (!should_interpolate_per_sample)
        {
            uint rp_instance = bitfieldExtract(primitive_attr.data[index].state,
                                               STATE_VERTEX_RENDER_PASS_INSTANCE_OFFSET,
                                               STATE_VERTEX_RENDER_PASS_INSTANCE_COUNT);

            uint z_shift = bitfieldExtract(registers.packed_z_shift, 4 * int(rp_instance), 4) * 8;

            // Don't demote lines unless we demote the entire render pass to single sampled.
            if (!line)
                primitive_attr.data[index].state |= 1u << STATE_BIT_SNAP_ATTRIBUTE;

            // Need to clamp in case primitive Z overflows. In that case, we get saturation.
            uint single_sampled_bucket_index = min(255u, za >> z_shift);
            uint bucket_index_hi = single_sampled_bucket_index / 32u;
            uint bucket_index_lo = single_sampled_bucket_index % 32u;

            // Could be clever and do waterfall atomic, but primitive count is low enough that we don't really care.
            // Each flat primitive falls into one of 256 buckets.
            // Every bucket should hold up to 64k depth values.
            // This matches well with LDS since we'll need to hold 2 KiB for negative mask.
            uint old_value = atomicOr(
                    heuristic.data.active_depth_range_mask_atomic[bucket_index_hi],
                    1u << bucket_index_lo);

            if (bitfieldExtract(old_value, int(bucket_index_lo), 1) == 0)
            {
                // We're the first thread that touched this bucket, we'll need to allocate a new indirect bucket.
                uint offset = atomicAdd(heuristic.data.single_sample_fixup_indirect.x, 1u);
                heuristic.data.depth_range_offset[offset] = single_sampled_bucket_index;
                if (offset == 0u)
                {
                    // First thread that allocated a bucket, setup the indirect args.
                    heuristic.data.single_sample_fixup_indirect.y = 1u;
                    heuristic.data.single_sample_fixup_indirect.z = 1u;
                }
            }
        }
    }
}

void main()
{
    uint index = gl_GlobalInvocationID.x;
    if (index < registers.num_primitives)
        main_inner(index);
}
